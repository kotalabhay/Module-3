{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwSlo4nidbbg"
   },
   "source": [
    "# The Word2Vec Model\n",
    "Welcome! Through the following lines of code, I have attempted to demonstrate the Word2Vec NN model using pytorch. You would actually need a much larger training corpus (>=10,000 words), than the one used in this code, in order to obtain a reasonably accurate set of word embeddings. You would also require a different type of softmax layer (hierarchical softmax) for faster computation. \n",
    "\n",
    "If you wish to modify certain parameters I would recommend following the guidelines given before each code cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4801,
     "status": "ok",
     "timestamp": 1601226337676,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "jK1FBHYWAYRH"
   },
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpBcHH-xhsap"
   },
   "source": [
    "### The Training Corpus\n",
    "\n",
    "\n",
    "*   Defines the list containing the sentences that the model is to be trained on.\n",
    "*   You can add/ edit/ delete sentences as per your preference, however DO NOT use punctuation or capitalization _(in order to avoid repetition of words)_.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1683,
     "status": "ok",
     "timestamp": 1601227270130,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "vegNKWAAtL4g"
   },
   "outputs": [],
   "source": [
    " corpus = ['he is a king','she is a queen',\n",
    "    'he is a man','i would like some mango juice',\n",
    "    'she is a woman','i would like some orange juice','i would like some apple juice','nairobi is the capital of kenya',\n",
    "    'delhi is the capital of india','oslo is the capital of norway'   \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-m_wMWalS8S"
   },
   "source": [
    "### Vocabulary and Word Indexing\n",
    "\n",
    "\n",
    "*   Splits corpus sentences into individual word sequences\n",
    "*   Generates a vocabulary of all words that occur in the training corpus\n",
    "*   Assigns indices to words\n",
    "*   Converts sequences of words into their respective index sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1220,
     "status": "ok",
     "timestamp": 1601228735046,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "VMDkC4cHUpH-"
   },
   "outputs": [],
   "source": [
    "#returns list of sentences and distinct word vocabulary\n",
    "def split_words(corpus):\n",
    "  tokens = []\n",
    "  vocab=set()\n",
    "  for x in corpus:\n",
    "    sep=x.split(' ')\n",
    "    tokens.append(sep)\n",
    "    vocab=vocab|set(sep)\n",
    "  vocab=list(vocab)\n",
    "  return tokens,vocab\n",
    "sequences,vocab = split_words(corpus)\n",
    "#maps word to index and index to word\n",
    "word_to_ind={}\n",
    "ind_to_word={}\n",
    "ind=0\n",
    "for word in vocab:\n",
    "  word_to_ind[word]=ind\n",
    "  ind_to_word[ind]=word\n",
    "  ind+=1\n",
    "#indexed sequences\n",
    "seq_inds=[[word_to_ind[word] for word in sequence] for sequence in sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLh9S1WkmIVE"
   },
   "source": [
    "### Window Selection and Context-Target Generation\n",
    "*  The window size and mode for context words' range can be chosen:-\n",
    "   *   Window mode bi-directional ('bi_dir') looks at words before and after the target word in sequence 2 times that of window size. \n",
    "   *   Window mode uni-directional ('uni_dir') looks at words only before the target word in sequence.\n",
    "*  The loop iterates over the entire corpus and applies the pair_words function on all sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1901,
     "status": "ok",
     "timestamp": 1601230653531,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "dC0M9X2fUwN7"
   },
   "outputs": [],
   "source": [
    "window_size=2\n",
    "mode='bi_dir'  \n",
    "\n",
    "\n",
    "def pair_words_single(seq_ind,window):\n",
    "  pairs=[]\n",
    "  for target_pos in range(len(seq_ind)):\n",
    "    context_start=max(target_pos-window,0)\n",
    "    context_end=target_pos\n",
    "    for context_pos in range(context_start,context_end):\n",
    "      pairs.append((seq_ind[context_pos],seq_ind[target_pos]))\n",
    "  return pairs\n",
    "def pair_words_dual(seq_ind,window):\n",
    "  pairs=[]\n",
    "  for target_pos in range(len(seq_ind)):\n",
    "    context_start=max(target_pos-window,0)\n",
    "    context_end=min(target_pos+window+1,len(seq_ind))\n",
    "    for context_pos in range(context_start,context_end):\n",
    "      if context_pos!=target_pos:\n",
    "        pairs.append((seq_ind[context_pos],seq_ind[target_pos]))\n",
    "  return pairs\n",
    "def pair_words(seq_ind,window_size=2,mode='bi_dir'):       #returns context-target pairs for a sequence of words/indices, given a window size(default=2) and mode(default=bi_dir)\n",
    "  if mode=='uni_dir':\n",
    "    return pair_words_single(seq_ind,window_size)\n",
    "  else:\n",
    "    return pair_words_dual(seq_ind,window_size)\n",
    "\n",
    "all_pairs=[]\n",
    "for seq_ind in seq_inds:                #loop iterates over all indexed sequences to compute all possible target-context pairs wrt each sentence\n",
    "  pairs_per_sequence=pair_words(seq_ind,window_size,mode)\n",
    "  for pair in pairs_per_sequence:\n",
    "    all_pairs.append(pair)\n",
    "all_pairs=np.array(all_pairs)  #context target pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAqrFYy_yPM5"
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "*   The embedding dimensions, number of epochs and learning rate can be chosen\n",
    "    *  Having many embedding dimensions helps if there is a large vocabulary\n",
    "*   Randomly initializes W1-Embedding matrix, W2-softmax weight\n",
    "*   We won't be multplying the W1 matrix by the one-hot context vector but rather chose the corresponding column from W1 using the word's index since this produces the same effect and is less computationally wasteful\n",
    "*   The F.log_softmax converts activation z2 into a softmax probability output. The  F.nll_loss computes loss of softmax output w.r.t target on-hot \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "executionInfo": {
     "elapsed": 16302,
     "status": "ok",
     "timestamp": 1601232154529,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "oGjYtCN9xWlX",
    "outputId": "45c7614c-692e-44e6-b29e-139920cb53ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 9.069747924804688\n",
      "Loss at epoch 50: 2.003243923187256\n",
      "Loss at epoch 100: 1.6007941961288452\n",
      "Loss at epoch 150: 1.537070870399475\n",
      "Loss at epoch 200: 1.5180995464324951\n",
      "Loss at epoch 250: 1.509350299835205\n",
      "Loss at epoch 300: 1.5043885707855225\n",
      "Loss at epoch 350: 1.5012295246124268\n",
      "Loss at epoch 400: 1.4990565776824951\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=20\n",
    "num_epochs = 401\n",
    "learning_rate = 0.003\n",
    "\n",
    "\n",
    "vocab_size=len(vocab)\n",
    "W1 = Variable(torch.randn(embedding_dim, vocab_size).float(), requires_grad=True)  #embedding matrix\n",
    "W2 = Variable(torch.randn(vocab_size, embedding_dim).float(), requires_grad=True)  #weights for softmax layer\n",
    "\n",
    "losses=[]\n",
    "for epoch in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for context, target in all_pairs:\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = W1[:,context]\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    l_epoch=loss_val/len(all_pairs)\n",
    "    losses.append(l_epoch)\n",
    "    if epoch % 50 == 0:                 # displays loss at every 50th epoch\n",
    "        print(f'Loss at epoch {epoch}: {l_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74wnbSX43YB-"
   },
   "source": [
    "### Displaying Loss Vs Epoch Number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1342,
     "status": "ok",
     "timestamp": 1601232418478,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "29WpzwmD4FSf",
    "outputId": "7c8a479c-e941-4d1b-fbbd-9f567248dc55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([epoch for epoch in range(num_epochs)],losses)\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5j4py8x6g0D"
   },
   "source": [
    "### Displaying the embedding vector\n",
    "* Using pandas dataframe to display the embedding matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "executionInfo": {
     "elapsed": 2740,
     "status": "ok",
     "timestamp": 1601232436930,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "XEBjLN0OxfCE",
    "outputId": "136f233f-521a-46f3-d515-5bad5ba90221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orange</th>\n",
       "      <th>is</th>\n",
       "      <th>oslo</th>\n",
       "      <th>would</th>\n",
       "      <th>delhi</th>\n",
       "      <th>like</th>\n",
       "      <th>mango</th>\n",
       "      <th>nairobi</th>\n",
       "      <th>india</th>\n",
       "      <th>he</th>\n",
       "      <th>...</th>\n",
       "      <th>juice</th>\n",
       "      <th>king</th>\n",
       "      <th>queen</th>\n",
       "      <th>man</th>\n",
       "      <th>woman</th>\n",
       "      <th>capital</th>\n",
       "      <th>a</th>\n",
       "      <th>the</th>\n",
       "      <th>i</th>\n",
       "      <th>some</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dim1</th>\n",
       "      <td>-0.718601</td>\n",
       "      <td>0.177996</td>\n",
       "      <td>1.464059</td>\n",
       "      <td>0.965732</td>\n",
       "      <td>-1.536459</td>\n",
       "      <td>-1.226401</td>\n",
       "      <td>-0.162112</td>\n",
       "      <td>1.062486</td>\n",
       "      <td>-0.438659</td>\n",
       "      <td>0.427767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710056</td>\n",
       "      <td>-1.382153</td>\n",
       "      <td>-0.400128</td>\n",
       "      <td>1.420530</td>\n",
       "      <td>0.060002</td>\n",
       "      <td>-0.193844</td>\n",
       "      <td>1.360463</td>\n",
       "      <td>0.697182</td>\n",
       "      <td>-1.107796</td>\n",
       "      <td>-1.458363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim2</th>\n",
       "      <td>0.070449</td>\n",
       "      <td>0.199047</td>\n",
       "      <td>-1.117345</td>\n",
       "      <td>0.055307</td>\n",
       "      <td>-1.585510</td>\n",
       "      <td>-1.457895</td>\n",
       "      <td>-1.333045</td>\n",
       "      <td>0.112163</td>\n",
       "      <td>-0.692107</td>\n",
       "      <td>0.056013</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.878259</td>\n",
       "      <td>0.590600</td>\n",
       "      <td>-0.693093</td>\n",
       "      <td>0.270018</td>\n",
       "      <td>0.794295</td>\n",
       "      <td>-1.218075</td>\n",
       "      <td>-0.045988</td>\n",
       "      <td>-1.225881</td>\n",
       "      <td>-0.091753</td>\n",
       "      <td>1.094044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim3</th>\n",
       "      <td>-0.558962</td>\n",
       "      <td>0.894313</td>\n",
       "      <td>1.236595</td>\n",
       "      <td>0.112119</td>\n",
       "      <td>-0.270379</td>\n",
       "      <td>-1.341065</td>\n",
       "      <td>-1.733485</td>\n",
       "      <td>0.249879</td>\n",
       "      <td>-0.449963</td>\n",
       "      <td>0.206287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323446</td>\n",
       "      <td>-0.803001</td>\n",
       "      <td>1.689490</td>\n",
       "      <td>-0.425242</td>\n",
       "      <td>-0.588031</td>\n",
       "      <td>-0.002898</td>\n",
       "      <td>0.702282</td>\n",
       "      <td>-0.901426</td>\n",
       "      <td>-0.802542</td>\n",
       "      <td>-0.092557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim4</th>\n",
       "      <td>0.323133</td>\n",
       "      <td>-1.027089</td>\n",
       "      <td>0.898222</td>\n",
       "      <td>-0.428105</td>\n",
       "      <td>-0.678932</td>\n",
       "      <td>2.001344</td>\n",
       "      <td>0.155436</td>\n",
       "      <td>1.355423</td>\n",
       "      <td>0.356824</td>\n",
       "      <td>-0.407350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458510</td>\n",
       "      <td>-1.195301</td>\n",
       "      <td>-0.377396</td>\n",
       "      <td>1.151165</td>\n",
       "      <td>-0.780778</td>\n",
       "      <td>0.191550</td>\n",
       "      <td>-0.094678</td>\n",
       "      <td>-0.331814</td>\n",
       "      <td>-0.339736</td>\n",
       "      <td>1.595707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim5</th>\n",
       "      <td>0.734673</td>\n",
       "      <td>0.593885</td>\n",
       "      <td>-0.681481</td>\n",
       "      <td>0.309500</td>\n",
       "      <td>0.175075</td>\n",
       "      <td>0.093503</td>\n",
       "      <td>1.155967</td>\n",
       "      <td>-1.014479</td>\n",
       "      <td>1.073746</td>\n",
       "      <td>-1.158621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.533552</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>-0.229285</td>\n",
       "      <td>0.339894</td>\n",
       "      <td>-0.000450</td>\n",
       "      <td>1.151553</td>\n",
       "      <td>-0.324833</td>\n",
       "      <td>0.883462</td>\n",
       "      <td>-0.544843</td>\n",
       "      <td>-1.489952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim6</th>\n",
       "      <td>-2.317082</td>\n",
       "      <td>0.179021</td>\n",
       "      <td>-0.450853</td>\n",
       "      <td>1.126296</td>\n",
       "      <td>-1.798695</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>-0.874035</td>\n",
       "      <td>-0.917235</td>\n",
       "      <td>-1.056295</td>\n",
       "      <td>-0.958606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.916634</td>\n",
       "      <td>-0.249868</td>\n",
       "      <td>-0.657378</td>\n",
       "      <td>0.484556</td>\n",
       "      <td>-0.979836</td>\n",
       "      <td>1.379127</td>\n",
       "      <td>-1.130657</td>\n",
       "      <td>-0.058161</td>\n",
       "      <td>0.894740</td>\n",
       "      <td>-0.691502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim7</th>\n",
       "      <td>-0.648600</td>\n",
       "      <td>0.712807</td>\n",
       "      <td>-0.812688</td>\n",
       "      <td>-1.760792</td>\n",
       "      <td>-0.361765</td>\n",
       "      <td>-1.276452</td>\n",
       "      <td>-1.487523</td>\n",
       "      <td>1.187946</td>\n",
       "      <td>1.033824</td>\n",
       "      <td>0.166655</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.219593</td>\n",
       "      <td>0.088364</td>\n",
       "      <td>-0.947047</td>\n",
       "      <td>-0.685236</td>\n",
       "      <td>-0.295114</td>\n",
       "      <td>0.947386</td>\n",
       "      <td>0.204770</td>\n",
       "      <td>0.729436</td>\n",
       "      <td>-0.692726</td>\n",
       "      <td>0.112763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim8</th>\n",
       "      <td>1.267705</td>\n",
       "      <td>0.307476</td>\n",
       "      <td>0.034075</td>\n",
       "      <td>-0.882373</td>\n",
       "      <td>-0.263934</td>\n",
       "      <td>-0.680435</td>\n",
       "      <td>0.120830</td>\n",
       "      <td>0.513260</td>\n",
       "      <td>1.487563</td>\n",
       "      <td>0.327581</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082723</td>\n",
       "      <td>-0.769587</td>\n",
       "      <td>1.277233</td>\n",
       "      <td>-2.561692</td>\n",
       "      <td>-0.170014</td>\n",
       "      <td>-0.209685</td>\n",
       "      <td>-0.937820</td>\n",
       "      <td>-0.907287</td>\n",
       "      <td>-0.340710</td>\n",
       "      <td>-0.542404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim9</th>\n",
       "      <td>0.512797</td>\n",
       "      <td>-0.554603</td>\n",
       "      <td>0.048291</td>\n",
       "      <td>-0.496424</td>\n",
       "      <td>0.878773</td>\n",
       "      <td>-0.245520</td>\n",
       "      <td>-0.915685</td>\n",
       "      <td>-0.121788</td>\n",
       "      <td>0.738846</td>\n",
       "      <td>0.161169</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000108</td>\n",
       "      <td>-0.063608</td>\n",
       "      <td>-0.666742</td>\n",
       "      <td>-0.045698</td>\n",
       "      <td>0.769146</td>\n",
       "      <td>0.982649</td>\n",
       "      <td>1.220369</td>\n",
       "      <td>0.660188</td>\n",
       "      <td>1.159985</td>\n",
       "      <td>0.651269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim10</th>\n",
       "      <td>-0.932376</td>\n",
       "      <td>-0.888959</td>\n",
       "      <td>-0.272708</td>\n",
       "      <td>0.397670</td>\n",
       "      <td>-1.100998</td>\n",
       "      <td>-0.066146</td>\n",
       "      <td>0.770620</td>\n",
       "      <td>0.216297</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>1.198908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312746</td>\n",
       "      <td>-1.307688</td>\n",
       "      <td>1.383398</td>\n",
       "      <td>-0.159594</td>\n",
       "      <td>0.056675</td>\n",
       "      <td>0.267677</td>\n",
       "      <td>0.239457</td>\n",
       "      <td>-1.798468</td>\n",
       "      <td>-1.260083</td>\n",
       "      <td>0.974359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim11</th>\n",
       "      <td>-0.591375</td>\n",
       "      <td>0.258552</td>\n",
       "      <td>-0.066250</td>\n",
       "      <td>0.167023</td>\n",
       "      <td>-0.128077</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>-0.439069</td>\n",
       "      <td>0.422863</td>\n",
       "      <td>-2.099838</td>\n",
       "      <td>-0.884543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065432</td>\n",
       "      <td>0.387133</td>\n",
       "      <td>-2.830139</td>\n",
       "      <td>-0.935924</td>\n",
       "      <td>-2.078355</td>\n",
       "      <td>-0.317197</td>\n",
       "      <td>0.541911</td>\n",
       "      <td>-2.297340</td>\n",
       "      <td>0.015055</td>\n",
       "      <td>0.806836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim12</th>\n",
       "      <td>-0.989623</td>\n",
       "      <td>1.278640</td>\n",
       "      <td>0.628775</td>\n",
       "      <td>-1.497053</td>\n",
       "      <td>0.801782</td>\n",
       "      <td>0.130582</td>\n",
       "      <td>0.531672</td>\n",
       "      <td>1.160261</td>\n",
       "      <td>0.313586</td>\n",
       "      <td>-0.743327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.825563</td>\n",
       "      <td>1.430267</td>\n",
       "      <td>0.962820</td>\n",
       "      <td>0.206693</td>\n",
       "      <td>-0.088607</td>\n",
       "      <td>-0.003579</td>\n",
       "      <td>1.778138</td>\n",
       "      <td>1.134307</td>\n",
       "      <td>0.830489</td>\n",
       "      <td>-1.408371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim13</th>\n",
       "      <td>1.024968</td>\n",
       "      <td>-1.299959</td>\n",
       "      <td>0.792634</td>\n",
       "      <td>-0.789129</td>\n",
       "      <td>-0.971612</td>\n",
       "      <td>-0.926052</td>\n",
       "      <td>-0.820299</td>\n",
       "      <td>-0.019069</td>\n",
       "      <td>0.430125</td>\n",
       "      <td>-0.826228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860693</td>\n",
       "      <td>0.485077</td>\n",
       "      <td>-0.694142</td>\n",
       "      <td>-0.327253</td>\n",
       "      <td>1.122176</td>\n",
       "      <td>1.148934</td>\n",
       "      <td>-0.995069</td>\n",
       "      <td>0.760513</td>\n",
       "      <td>-1.580653</td>\n",
       "      <td>0.530072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim14</th>\n",
       "      <td>0.343806</td>\n",
       "      <td>-0.765212</td>\n",
       "      <td>-0.192243</td>\n",
       "      <td>-0.517875</td>\n",
       "      <td>-1.327895</td>\n",
       "      <td>-0.105530</td>\n",
       "      <td>-0.775403</td>\n",
       "      <td>0.699463</td>\n",
       "      <td>2.279148</td>\n",
       "      <td>0.804748</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036012</td>\n",
       "      <td>-0.555807</td>\n",
       "      <td>-0.905646</td>\n",
       "      <td>0.480220</td>\n",
       "      <td>0.650622</td>\n",
       "      <td>-0.186779</td>\n",
       "      <td>0.523220</td>\n",
       "      <td>0.047210</td>\n",
       "      <td>-2.253336</td>\n",
       "      <td>0.018570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim15</th>\n",
       "      <td>2.189666</td>\n",
       "      <td>1.140431</td>\n",
       "      <td>0.236260</td>\n",
       "      <td>1.099449</td>\n",
       "      <td>-0.620115</td>\n",
       "      <td>-0.531221</td>\n",
       "      <td>0.093768</td>\n",
       "      <td>-1.409760</td>\n",
       "      <td>1.528984</td>\n",
       "      <td>-0.288123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.497322</td>\n",
       "      <td>-0.858807</td>\n",
       "      <td>1.241363</td>\n",
       "      <td>-0.173212</td>\n",
       "      <td>-0.641644</td>\n",
       "      <td>-0.145506</td>\n",
       "      <td>1.154056</td>\n",
       "      <td>0.197504</td>\n",
       "      <td>0.437021</td>\n",
       "      <td>-0.119708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim16</th>\n",
       "      <td>0.376277</td>\n",
       "      <td>-1.703594</td>\n",
       "      <td>-0.445748</td>\n",
       "      <td>-0.896635</td>\n",
       "      <td>-0.693893</td>\n",
       "      <td>-0.650977</td>\n",
       "      <td>1.169395</td>\n",
       "      <td>-0.069730</td>\n",
       "      <td>0.015959</td>\n",
       "      <td>-0.686748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884052</td>\n",
       "      <td>1.345403</td>\n",
       "      <td>-0.771399</td>\n",
       "      <td>-0.086215</td>\n",
       "      <td>0.737459</td>\n",
       "      <td>0.402127</td>\n",
       "      <td>-1.105026</td>\n",
       "      <td>1.711337</td>\n",
       "      <td>-0.838505</td>\n",
       "      <td>1.148814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim17</th>\n",
       "      <td>-0.169378</td>\n",
       "      <td>-0.497769</td>\n",
       "      <td>-0.663797</td>\n",
       "      <td>-2.882021</td>\n",
       "      <td>-0.737063</td>\n",
       "      <td>1.463705</td>\n",
       "      <td>-0.073598</td>\n",
       "      <td>-0.076488</td>\n",
       "      <td>1.175844</td>\n",
       "      <td>-1.600814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817869</td>\n",
       "      <td>-2.276454</td>\n",
       "      <td>-0.238008</td>\n",
       "      <td>-2.021876</td>\n",
       "      <td>-1.120396</td>\n",
       "      <td>-0.707735</td>\n",
       "      <td>0.410434</td>\n",
       "      <td>0.472938</td>\n",
       "      <td>0.057783</td>\n",
       "      <td>-0.066373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim18</th>\n",
       "      <td>0.054825</td>\n",
       "      <td>-1.331474</td>\n",
       "      <td>-0.243254</td>\n",
       "      <td>1.095723</td>\n",
       "      <td>-0.549969</td>\n",
       "      <td>-0.078591</td>\n",
       "      <td>0.457891</td>\n",
       "      <td>-2.178646</td>\n",
       "      <td>0.433276</td>\n",
       "      <td>-0.825976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898830</td>\n",
       "      <td>-0.654737</td>\n",
       "      <td>-1.174373</td>\n",
       "      <td>-0.573421</td>\n",
       "      <td>-1.050899</td>\n",
       "      <td>0.647973</td>\n",
       "      <td>-0.184198</td>\n",
       "      <td>1.399491</td>\n",
       "      <td>0.404371</td>\n",
       "      <td>0.575562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim19</th>\n",
       "      <td>0.113573</td>\n",
       "      <td>-0.272916</td>\n",
       "      <td>1.254342</td>\n",
       "      <td>0.769636</td>\n",
       "      <td>1.306250</td>\n",
       "      <td>-1.388279</td>\n",
       "      <td>-0.988806</td>\n",
       "      <td>1.470852</td>\n",
       "      <td>-1.298910</td>\n",
       "      <td>-0.451146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043171</td>\n",
       "      <td>0.865955</td>\n",
       "      <td>-0.714813</td>\n",
       "      <td>-1.635872</td>\n",
       "      <td>0.547102</td>\n",
       "      <td>1.295986</td>\n",
       "      <td>0.412248</td>\n",
       "      <td>0.147261</td>\n",
       "      <td>0.956481</td>\n",
       "      <td>-0.960934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim20</th>\n",
       "      <td>1.171640</td>\n",
       "      <td>0.270978</td>\n",
       "      <td>-0.121637</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>-0.599318</td>\n",
       "      <td>-0.984974</td>\n",
       "      <td>0.467626</td>\n",
       "      <td>1.739039</td>\n",
       "      <td>0.226595</td>\n",
       "      <td>0.246195</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705533</td>\n",
       "      <td>1.216619</td>\n",
       "      <td>-0.091132</td>\n",
       "      <td>0.406100</td>\n",
       "      <td>-0.037199</td>\n",
       "      <td>-1.114644</td>\n",
       "      <td>0.412273</td>\n",
       "      <td>0.162390</td>\n",
       "      <td>1.472645</td>\n",
       "      <td>-0.577806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         orange        is      oslo     would     delhi      like     mango  \\\n",
       "dim1  -0.718601  0.177996  1.464059  0.965732 -1.536459 -1.226401 -0.162112   \n",
       "dim2   0.070449  0.199047 -1.117345  0.055307 -1.585510 -1.457895 -1.333045   \n",
       "dim3  -0.558962  0.894313  1.236595  0.112119 -0.270379 -1.341065 -1.733485   \n",
       "dim4   0.323133 -1.027089  0.898222 -0.428105 -0.678932  2.001344  0.155436   \n",
       "dim5   0.734673  0.593885 -0.681481  0.309500  0.175075  0.093503  1.155967   \n",
       "dim6  -2.317082  0.179021 -0.450853  1.126296 -1.798695  0.499990 -0.874035   \n",
       "dim7  -0.648600  0.712807 -0.812688 -1.760792 -0.361765 -1.276452 -1.487523   \n",
       "dim8   1.267705  0.307476  0.034075 -0.882373 -0.263934 -0.680435  0.120830   \n",
       "dim9   0.512797 -0.554603  0.048291 -0.496424  0.878773 -0.245520 -0.915685   \n",
       "dim10 -0.932376 -0.888959 -0.272708  0.397670 -1.100998 -0.066146  0.770620   \n",
       "dim11 -0.591375  0.258552 -0.066250  0.167023 -0.128077  0.461538 -0.439069   \n",
       "dim12 -0.989623  1.278640  0.628775 -1.497053  0.801782  0.130582  0.531672   \n",
       "dim13  1.024968 -1.299959  0.792634 -0.789129 -0.971612 -0.926052 -0.820299   \n",
       "dim14  0.343806 -0.765212 -0.192243 -0.517875 -1.327895 -0.105530 -0.775403   \n",
       "dim15  2.189666  1.140431  0.236260  1.099449 -0.620115 -0.531221  0.093768   \n",
       "dim16  0.376277 -1.703594 -0.445748 -0.896635 -0.693893 -0.650977  1.169395   \n",
       "dim17 -0.169378 -0.497769 -0.663797 -2.882021 -0.737063  1.463705 -0.073598   \n",
       "dim18  0.054825 -1.331474 -0.243254  1.095723 -0.549969 -0.078591  0.457891   \n",
       "dim19  0.113573 -0.272916  1.254342  0.769636  1.306250 -1.388279 -0.988806   \n",
       "dim20  1.171640  0.270978 -0.121637  0.032051 -0.599318 -0.984974  0.467626   \n",
       "\n",
       "        nairobi     india        he  ...     juice      king     queen  \\\n",
       "dim1   1.062486 -0.438659  0.427767  ...  0.710056 -1.382153 -0.400128   \n",
       "dim2   0.112163 -0.692107  0.056013  ... -0.878259  0.590600 -0.693093   \n",
       "dim3   0.249879 -0.449963  0.206287  ...  0.323446 -0.803001  1.689490   \n",
       "dim4   1.355423  0.356824 -0.407350  ...  0.458510 -1.195301 -0.377396   \n",
       "dim5  -1.014479  1.073746 -1.158621  ... -0.533552  0.001511 -0.229285   \n",
       "dim6  -0.917235 -1.056295 -0.958606  ... -0.916634 -0.249868 -0.657378   \n",
       "dim7   1.187946  1.033824  0.166655  ... -1.219593  0.088364 -0.947047   \n",
       "dim8   0.513260  1.487563  0.327581  ... -0.082723 -0.769587  1.277233   \n",
       "dim9  -0.121788  0.738846  0.161169  ... -2.000108 -0.063608 -0.666742   \n",
       "dim10  0.216297  0.052948  1.198908  ...  0.312746 -1.307688  1.383398   \n",
       "dim11  0.422863 -2.099838 -0.884543  ...  0.065432  0.387133 -2.830139   \n",
       "dim12  1.160261  0.313586 -0.743327  ...  0.825563  1.430267  0.962820   \n",
       "dim13 -0.019069  0.430125 -0.826228  ...  0.860693  0.485077 -0.694142   \n",
       "dim14  0.699463  2.279148  0.804748  ... -0.036012 -0.555807 -0.905646   \n",
       "dim15 -1.409760  1.528984 -0.288123  ... -0.497322 -0.858807  1.241363   \n",
       "dim16 -0.069730  0.015959 -0.686748  ...  0.884052  1.345403 -0.771399   \n",
       "dim17 -0.076488  1.175844 -1.600814  ...  0.817869 -2.276454 -0.238008   \n",
       "dim18 -2.178646  0.433276 -0.825976  ...  0.898830 -0.654737 -1.174373   \n",
       "dim19  1.470852 -1.298910 -0.451146  ...  0.043171  0.865955 -0.714813   \n",
       "dim20  1.739039  0.226595  0.246195  ... -0.705533  1.216619 -0.091132   \n",
       "\n",
       "            man     woman   capital         a       the         i      some  \n",
       "dim1   1.420530  0.060002 -0.193844  1.360463  0.697182 -1.107796 -1.458363  \n",
       "dim2   0.270018  0.794295 -1.218075 -0.045988 -1.225881 -0.091753  1.094044  \n",
       "dim3  -0.425242 -0.588031 -0.002898  0.702282 -0.901426 -0.802542 -0.092557  \n",
       "dim4   1.151165 -0.780778  0.191550 -0.094678 -0.331814 -0.339736  1.595707  \n",
       "dim5   0.339894 -0.000450  1.151553 -0.324833  0.883462 -0.544843 -1.489952  \n",
       "dim6   0.484556 -0.979836  1.379127 -1.130657 -0.058161  0.894740 -0.691502  \n",
       "dim7  -0.685236 -0.295114  0.947386  0.204770  0.729436 -0.692726  0.112763  \n",
       "dim8  -2.561692 -0.170014 -0.209685 -0.937820 -0.907287 -0.340710 -0.542404  \n",
       "dim9  -0.045698  0.769146  0.982649  1.220369  0.660188  1.159985  0.651269  \n",
       "dim10 -0.159594  0.056675  0.267677  0.239457 -1.798468 -1.260083  0.974359  \n",
       "dim11 -0.935924 -2.078355 -0.317197  0.541911 -2.297340  0.015055  0.806836  \n",
       "dim12  0.206693 -0.088607 -0.003579  1.778138  1.134307  0.830489 -1.408371  \n",
       "dim13 -0.327253  1.122176  1.148934 -0.995069  0.760513 -1.580653  0.530072  \n",
       "dim14  0.480220  0.650622 -0.186779  0.523220  0.047210 -2.253336  0.018570  \n",
       "dim15 -0.173212 -0.641644 -0.145506  1.154056  0.197504  0.437021 -0.119708  \n",
       "dim16 -0.086215  0.737459  0.402127 -1.105026  1.711337 -0.838505  1.148814  \n",
       "dim17 -2.021876 -1.120396 -0.707735  0.410434  0.472938  0.057783 -0.066373  \n",
       "dim18 -0.573421 -1.050899  0.647973 -0.184198  1.399491  0.404371  0.575562  \n",
       "dim19 -1.635872  0.547102  1.295986  0.412248  0.147261  0.956481 -0.960934  \n",
       "dim20  0.406100 -0.037199 -1.114644  0.412273  0.162390  1.472645 -0.577806  \n",
       "\n",
       "[20 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "embedding_matrix=np.array(W1.data)\n",
    "\n",
    "dfl=pd.DataFrame(embedding_matrix)\n",
    "dfl.columns=[ind_to_word[i] for i in range(vocab_size)]\n",
    "dfl.index=['dim'+str(i+1) for i in range(embedding_dim)]\n",
    "print('Embedding Matrix')\n",
    "dfl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOtcvOpU5oIm"
   },
   "source": [
    "### Conclusion:\n",
    "If you've used the default/pre-defined parameters you might have noticed that the embedding vector isn't quite accurate. This is because, the model is limited in it's ability to generalise over a large corpus of training sentences.\n",
    "\n",
    "This can be understood by the fact even us humans would find it hard to pick up similar words and analogies given a limited set of sentences from a completely unfamiliar language.\n",
    "\n",
    "\n",
    "\n",
    "Thank You!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPRVxHGO7y2mp8TjoPQOP8G",
   "collapsed_sections": [],
   "name": "Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
